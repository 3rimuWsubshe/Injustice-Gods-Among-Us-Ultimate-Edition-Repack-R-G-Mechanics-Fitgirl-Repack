-- Pool Allocator
--
-- The pool allocator allocate chunks from fixed contiguous buffer of many chunks,
-- allocations pops a free chunk from the pool and deallocations pushes a chunk back.
-- It works by using a single linked list of free chunks.
--
-- The purpose of this allocator is to have very fast allocations with almost
-- no runtime cost when the maximum used space is known ahead.
--
-- Reallocations and deallocations free space (unlikely the Arena allocator).
-- Allocations greater than the chunk size will always fails.
--
-- The implementation is based on
-- https://www.gingerbill.org/article/2019/02/16/memory-allocation-strategies-004/

require 'allocators.interface'

## local make_pool_allocator = generalize(function(T, SIZE)
  ##[[
  static_assert(traits.is_type(T), 'T must be a type')
  local CHUNK_SIZE = T.size
  local CHUNK_ALIGN = math.max(T.align, primtypes.pointer.size)
  static_assert(traits.is_number(SIZE), 'size must be a number')
  static_assert(SIZE * CHUNK_SIZE > 0, 'size must be greater than 0')
  static_assert(CHUNK_SIZE >= primtypes.pointer.size, 'chunk size must be at least a pointer in size')
  ]]

  local PoolFreeNode = @record{next: *PoolFreeNode}
  local PoolChunk <aligned(#[CHUNK_ALIGN]#)> = @record {data: [#[CHUNK_SIZE]#]byte}
  local PoolAllocatorT = @record{
    initialized: boolean,
    head: *PoolFreeNode,
    buffer: [#[SIZE]#]PoolChunk
  }

  -- Free all allocations.
  function PoolAllocatorT:dealloc_all(): void <noinline>
    self.head = nilptr
    -- link all free nodes in reverse order
    for i:isize=#self.buffer-1,0,-1 do
      local node: *PoolFreeNode = (@*PoolFreeNode)(&self.buffer[i])
      node.next = self.head
      self.head = node
    end
  end

  -- Initialize the pool allocator.
  -- There is not need to call this if zero initialized, it's called automatically on first alloc.
  function PoolAllocatorT:init(): void
    self.initialized = true
    self:dealloc_all()
  end

  function PoolAllocatorT:alloc(size: usize): pointer
    if unlikely(size > #PoolChunk) then
      return nilptr
    end
    -- get the latest free node
    local node: *PoolFreeNode = self.head
    -- the node will be nilptr if not initialized or out of memory
    if unlikely(node == nilptr) then
      if not self.initialized then
        -- first initialization
        self:init()
        node = self.head
      else
        -- out of memory
        return nilptr
      end
    end
    -- pop free node
    self.head = node.next
    -- the node is now actually the allocated chunk
    return node
  end

  function PoolAllocatorT:dealloc(p: pointer): void
    if unlikely(p == nilptr) then return end
    -- is this pointer really valid?
    local offset: usize = (@usize)(p) - (@usize)(&self.buffer[0])
    check(offset // #PoolChunk < #self.buffer and offset % #PoolChunk == 0, 'pointer not in buffer bounds')
    -- push free node
    local node: *PoolFreeNode = (@*PoolFreeNode)(p)
    node.next = self.head
    self.head = node
  end

  function PoolAllocatorT:realloc(p: pointer, newsize: usize, oldsize: usize): pointer
    if unlikely(p == nilptr) then
      return self:alloc(newsize)
    elseif unlikely(newsize == 0) then
      self:dealloc(p)
      return nilptr
    elseif unlikely(newsize > #PoolChunk) then
      return nilptr
    else
      return p
    end
  end

  ## implement_allocator_interface(PoolAllocatorT)

  ## return PoolAllocatorT
## end)

global PoolAllocator: type = #[make_pool_allocator]#
