-- Arena Allocator
--
-- The arena allocator, sometimes also know as linear, monotonic or region allocator,
-- allocates everything from fixed size contiguous buffer by incrementing 
-- an offset every new allocation.
--
-- The purpose of this allocator is to have very fast allocations with almost
-- no runtime cost when the maximum used space is known ahead
-- and to quickly deallocate many allocated objects at once with almost no runtime cost too.
--
-- Reallocations and deallocations does not free space unless once for the last recent allocation.
-- To free space `dealloc_all` should be called when all operations on its allocations are finished.
--
-- The allocator buffer will reside on the stack when declared inside a function,
-- or on the static memory storage when declared in a top scope,
-- or on the heap if allocated by the general allocator.
--
-- When declaring on the stack there is no need to perform deallocations at the end of the scope,
-- just leave the scope ends to have a quick cleanup.
-- Also take care to not use a large buffer on the stack,
-- or the program may crash with not enough stack space,
-- on some system for example the stack is limited to 1MB.
--
-- By default allocations are aligned to 8 bytes unless explicit told otherwise.
-- By default when there is not enough space a nil pointer is returned on allocations,
-- this can be changed to runtime errors by setting `error_on_failure` to true.
-- Remember to use the proper alignment for the allocated objects to have fast memory access.
--
-- The implementation is based on
-- https://www.gingerbill.org/article/2019/02/08/memory-allocation-strategies-002/

require 'allocators.interface'

local function align_forward(addr: usize, align: usize): usize <inline>
  return (addr + (align-1)) & ~(align-1)
end

local function memcpy(dest: pointer, src: pointer, n: csize): pointer <cimport,cinclude'<string.h>',nodecl> end

## local make_arena_allocator = generalize(function(SIZE, ALIGN, error_on_failure)
  ## ALIGN = ALIGN or 8
  ## static_assert(SIZE % ALIGN == 0, 'ArenaAllocator: size must be multiple of align')
  ## static_assert(ALIGN & (ALIGN-1) == 0, 'ArenaAllocator: align must be a power of two')

  local SIZE <comptime> = #[SIZE]#
  local ALIGN <comptime> = #[ALIGN]#
  local ArenaAllocatorT = @record{
    prev_offset: usize,
    curr_offset: usize,
    buffer: byte[SIZE]
  }

  -- Free all allocations.
  function ArenaAllocatorT:dealloc_all()
    self.prev_offset = 0
    self.curr_offset = 0
  end

  function ArenaAllocatorT:alloc(size: usize): pointer
    local base: usize = (@usize)(&self.buffer[0])
    local offset: usize = align_forward(base + self.curr_offset, ALIGN) - base
    local next_offset: usize = offset + size
    if unlikely(next_offset > SIZE) then
      ## if error_on_failure then
        error('ArenaAllocator.alloc: out of memory')
      ## end
      return nilptr
    end
    local p: pointer = &self.buffer[offset]
    self.prev_offset = offset
    self.curr_offset = next_offset
    return p
  end

  function ArenaAllocatorT:dealloc(p: pointer)
    if unlikely(p == nilptr) then return end
    -- get offset for this pointer
    local offset: usize = (@usize)(p) - (@usize)(&self.buffer[0])
    check(offset < SIZE, 'ArenaAllocator.dealloc: pointer not in buffer of bounds')
    -- we can only dealloc the most recent allocation once
    -- any other allocation we can do nothing about
    if likely(offset == self.prev_offset) then
      self.curr_offset = offset
    end
  end

  function ArenaAllocatorT:realloc(p: pointer, newsize: usize, oldsize: usize): pointer
    if unlikely(p == nilptr) then
      return self:alloc(newsize)
    elseif unlikely(newsize == 0) then
      self:dealloc(p)
      return nilptr
    end
    local offset: usize = (@usize)(p) - (@usize)(&self.buffer[0])
    check(offset < SIZE, 'ArenaAllocator.realloc: pointer not in buffer of bounds')
    if likely(offset == self.prev_offset) then -- is the very last allocation?
      -- we can just update the offset here to grow or shrink
      local next_offset: usize = offset + newsize
      if unlikely(next_offset > SIZE) then
        ## if error_on_failure then
          error('ArenaAllocator.realloc: out of memory')
        ## end
        return nilptr
      end
      self.curr_offset = next_offset
      return p
    elseif newsize > oldsize then -- growing
      -- when growing we need to move to a new allocation
      if unlikely(newsize == 0) then return nilptr end
      local newp: pointer = self:alloc(newsize)
      if likely(newp ~= nilptr and p ~= nilptr and oldsize ~= 0) then
        -- copy the mem to the new location
        memcpy(newp, p, oldsize)
      end
      -- no dealloc is done on old pointer because it's not possible in this allocator
      return newp
    else -- same size or shrinking, can return the same pointer
      return p
    end
  end

  ## implement_allocator_interface(ArenaAllocatorT)

  ## return ArenaAllocatorT
## end)

global ArenaAllocator: type = #[make_arena_allocator]#
